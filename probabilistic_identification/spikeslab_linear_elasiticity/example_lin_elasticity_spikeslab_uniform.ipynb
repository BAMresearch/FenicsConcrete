{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "parentdir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.join(os.getcwd(), 'example_lin_elasticity_spikeslab.ipynb'))))\n",
    "sys.path.append(parentdir)\n",
    "#print(parentdir)\n",
    "import numpy as np\n",
    "import fenicsX_concrete\n",
    "import json \n",
    "\n",
    "#with open('test_config.json', 'r') as f: \n",
    "#    json_object = json.loads(f.read()) \n",
    "\n",
    "# Adding sensors to the problem definition.\n",
    "def add_sensor(length, breadth, _problem, _dirichlet_bdy, _sensors_num_edge_hor, _sensors_num_edge_ver): \n",
    "    sensor = []\n",
    "    if _dirichlet_bdy == 0: #'left'\n",
    "        for i in range(_sensors_num_edge_hor): \n",
    "            #print((p['length']*(i+1))/_sensors_num_edge_hor) #p['length']\n",
    "            x_coord = (length*(i+1))/_sensors_num_edge_hor\n",
    "            sensor.append(fenicsX_concrete.sensors.DisplacementSensor(np.array([[x_coord, 0, 0]]), 'top')) #1/20\n",
    "            sensor.append(fenicsX_concrete.sensors.DisplacementSensor(np.array([[x_coord, breadth, 0]]), 'bottom'))\n",
    "        \n",
    "        for i in range(_sensors_num_edge_ver):\n",
    "            #print((p['breadth']*(i+1))/(_sensors_num_edge_ver+1))\n",
    "            y_coord = (breadth*(i+1))/(_sensors_num_edge_ver+1)\n",
    "            sensor.append(fenicsX_concrete.sensors.DisplacementSensor(np.array([[length, y_coord, 0]]), 'right'))\n",
    "\n",
    "        for i in range(len(sensor)):\n",
    "            _problem.add_sensor(sensor[i])\n",
    "        return len(sensor)\n",
    "    \n",
    "\"\"\" def store_sensor_data(_problem):\n",
    "    mydict = {}\n",
    "    for i in _problem.sensors:\n",
    "       sensor = {i :    \n",
    "        {\"alphabetical_position\" : problem.sensors[i].alphabetical_position,\n",
    "         \"where\" : problem.sensors[i].where[0].tolist(),\n",
    "         \"data\" : problem.sensors[i].data[0].tolist()}\n",
    "        } \n",
    "       mydict.update(sensor)\n",
    "    json_string = json.dumps(mydict , indent = 3)\n",
    "    with open(json_object.get('Data').get('sensor_data'), 'w') as f:\n",
    "        f.write(json_string)  \"\"\"\n",
    "    \n",
    "    \n",
    "def run_test(exp, prob, dirichlet_bdy, load, sensor_flag = 0):\n",
    "    #if dirichlet_bdy == 0:\n",
    "    #    dirichlet_bdy = 'left'\n",
    "    #prob.p.dirichlet_bdy = dirichlet_bdy\n",
    "    #exp.p.dirichlet_bdy = dirichlet_bdy\n",
    "    #prob.p.load = load\n",
    "    #prob.experiment.bcs = prob.experiment.create_displ_bcs(prob.experiment.V)\n",
    "    #prob.apply_neumann_bc()\n",
    "    #prob.calculate_bilinear_form()\n",
    "    prob.solve()\n",
    "    #prob.pv_plot(\"Displacement.xdmf\")\n",
    "    #store_sensor_data(prob)\n",
    "    if sensor_flag == 1:\n",
    "        counter=0\n",
    "        displacement_at_sensors = np.zeros((len(prob.sensors),2))\n",
    "        for i in prob.sensors:\n",
    "            displacement_at_sensors[counter] = prob.sensors[i].data[-1]\n",
    "            counter += 1\n",
    "        #prob.sensors = fenicsX_concrete.sensors.Sensors()\n",
    "        return displacement_at_sensors#.flatten()\n",
    "    elif sensor_flag == 0:\n",
    "        return prob.displacement.x.array\n",
    "\n",
    "def add_noise_to_data(clean_data, no_of_sensors):\n",
    "    #max_disp = np.amax(np.absolute(clean_data))\n",
    "    #min_disp = np.amin(np.absolute(clean_data))\n",
    "    #print('Max', max_disp, 'Min', min_disp)\n",
    "    #if json_object.get('MCMC').get('Error'):\n",
    "    #    return clean_data + np.random.normal(0, 0.01 * min_disp, no_of_sensors) ################################################################\n",
    "    #else:\n",
    "    return clean_data + np.random.normal(0, 1e-5, no_of_sensors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation using linear elasticity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "p = fenicsX_concrete.Parameters()  # using the current default values\n",
    "p['bc_setting'] = 'free'\n",
    "p['degree'] = 1\n",
    "p['num_elements_length'] = 25\n",
    "p['num_elements_breadth'] = 5\n",
    "p['dim'] = 2\n",
    "# Uncertainty type:\n",
    "# 0: Constant E and nu fields.\n",
    "# 1: Random E and nu fields.\n",
    "# 2: Linear Springs.\n",
    "# 3: Torsion Springs\n",
    "p['uncertainties'] = [0]\n",
    "#p['k_x'] = 0.5e7\n",
    "#p['k_y'] = 0.5e7\n",
    "\n",
    "p['constitutive'] = 'isotropic' #'orthotropic' \n",
    "p['nu'] = 0.28\n",
    "\n",
    "# Kgmms⁻2/mm², mm, kg, sec, N\n",
    "p['length'] = 1#1000\n",
    "p['breadth'] = 0.05#50\n",
    "\n",
    "p['load'] = [0, -2e7] #[1e3, 0] \n",
    "p['lower_limit'] = 0.9*p['length']\n",
    "p['upper_limit'] = p['length']\n",
    "p['rho'] = 7750 #7750e-9 #kg/mm³\n",
    "p['g'] = 9.81 #9.81e3 #mm/s² for units to be consistent g must be given in m/s².\n",
    "p['E'] = 210e9 #200e6 #Kgmms⁻2/mm² \n",
    "\n",
    "p['dirichlet_bdy'] = 'left'\n",
    "p['body_force'] = False\n",
    "\n",
    "sensors_num_edge_hor = 5\n",
    "sensors_num_edge_ver = 4\n",
    "\n",
    "experiment = fenicsX_concrete.concreteSlabExperiment(p)         # Specifies the domain, discretises it and apply Dirichlet BCs\n",
    "problem = fenicsX_concrete.LinearElasticity(experiment, p)      # Specifies the material law and weak forms.\n",
    "\n",
    "#Adding sensors to the problem definition.\n",
    "test1_sensors_total_num = add_sensor(p['length'], p['breadth'], problem, 0, sensors_num_edge_hor, sensors_num_edge_ver)\n",
    "sensor_positions = np.zeros((test1_sensors_total_num, 3))\n",
    "counter = 0\n",
    "for i in problem.sensors:\n",
    "    sensor_positions[counter] = problem.sensors[i].where[0]\n",
    "    counter += 1\n",
    "\n",
    "#Sparse data (with sensors)\n",
    "\n",
    "temperature_data = np.arange(15, 35, 5) # in degree celsius\n",
    "youngs_modulus = np.zeros(len(temperature_data))\n",
    "data = np.zeros((2*test1_sensors_total_num, len(temperature_data)))\n",
    "for counter, temp in enumerate(temperature_data):\n",
    "    youngs_modulus[counter] = (235 - 0.04 * temp ** 2)*10**9\n",
    "    problem.E.value = youngs_modulus[counter] #Remember problem.p.E is still at its initial value.\n",
    "\n",
    "    #Adding sensors to the problem definition.\n",
    "    #test1_sensors_total_num = add_sensor(problem, 0, sensors_num_edge_hor, sensors_num_edge_ver)\n",
    "    #sensor_positions = np.zeros((test1_sensors_total_num, 3))\n",
    "    #counter = 0\n",
    "    #for i in problem.sensors:\n",
    "    #    sensor_positions[counter] = problem.sensors[i].where[0]\n",
    "    #    counter += 1\n",
    "\n",
    "    test1_data = run_test(experiment, problem, 0, p['load'] , 1)\n",
    "    test1_x_component = add_noise_to_data(test1_data[:,0], test1_sensors_total_num)\n",
    "    test1_y_component = add_noise_to_data(test1_data[:,1], test1_sensors_total_num)\n",
    "\n",
    "    # Data stored in the form of XYXY components.\n",
    "    data[:,counter] = np.vstack((test1_x_component, test1_y_component)).T.flatten()\n",
    "\n",
    "displacement_data = data.flatten('F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Prior Distributions from .json File. \n",
    "import json\n",
    "with open('parameters_linear_uniform1.json', 'r') as f: \n",
    "    json_object = json.loads(f.read()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = len(json_object.get('parameters')) \n",
    "nwalkers = ndim*2+1\n",
    "\n",
    "from scipy.stats import norm, uniform #bernoulli, invgamma, halfcauchy, \n",
    "start_parameters = np.zeros((nwalkers, ndim))\n",
    "counter = 0\n",
    "\n",
    "# This loop reads the parameters from the json file and samples from the prior distributions\n",
    "for index, parameter in enumerate(json_object.get('parameters')):\n",
    "    #if parameter['prior'][0] == 'Bernoulli':\n",
    "    #    start_parameters[:, index] = bernoulli.rvs(p = parameter['prior'][1][\"p\"], size=nwalkers)\n",
    "    #elif parameter['prior'][0] == 'Spike-Slab':\n",
    "    #    for hyperparameter in parameter['hyperparameters']:\n",
    "    #        for ind, param in enumerate(json_object.get('parameters')):\n",
    "    #            if hyperparameter == param['name']:\n",
    "    #                lmbda = start_parameters[:, ind]\n",
    "    #                break\n",
    "    #    start_parameters[:, index] = scaler*lmbda*norm.rvs(loc = parameter['prior'][1][\"mean\"], scale = parameter['prior'][1][\"std_dev\"], size=nwalkers) \n",
    "    if parameter['prior'][0] == 'Normal': \n",
    "        start_parameters[:, index] = norm.rvs(loc = parameter['prior'][1][\"mean\"], scale = parameter['prior'][1][\"std_dev\"], size=nwalkers) \n",
    "    elif parameter['prior'][0] == 'Uniform':   \n",
    "        start_parameters[:, index] = uniform.rvs(loc = parameter['prior'][1][\"lower_bound\"], scale = parameter['prior'][1][\"lower_bound\"] + parameter['prior'][1][\"upper_bound\"], size=nwalkers)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for inverse problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_functions = np.zeros((5,len(temperature_data)))\n",
    "temperature_functions[0] = np.ones(len(temperature_data))\n",
    "temperature_functions[1] = np.square(temperature_data)\n",
    "temperature_functions[2] = temperature_data\n",
    "temperature_functions[3] = np.log(temperature_data)\n",
    "temperature_functions[4] = np.exp(-temperature_data)\n",
    "\n",
    "def log_likelihood(theta, displacement_data, _std_dev_noise, temperature_functions):\n",
    "    youngs_modulus_proposal = 235*temperature_functions[0] + \\\n",
    "    (0 if theta[0] <= 0.5 else 1)*theta[1]* temperature_functions[1] + \\\n",
    "    (0 if theta[2] <= 0.5 else 1)*theta[3]* temperature_functions[2] +\\\n",
    "    (0 if theta[4] <= 0.5 else 1)*theta[5]* temperature_functions[3] +\\\n",
    "    (0 if theta[6] <= 0.5 else 1)*theta[7]* temperature_functions[4]\n",
    "    \n",
    "    if  np.any(youngs_modulus_proposal < 15):\n",
    "        return -np.inf\n",
    "\n",
    "    displacement_model = np.zeros((2*test1_sensors_total_num, len(temperature_data)))\n",
    "    for counter, value in enumerate(youngs_modulus_proposal):\n",
    "        problem.E.value = value*10**9 #Remember problem.p.E is still at its initial value.\n",
    "        test1_data = run_test(experiment, problem, 0, p['load'] , 1)\n",
    "        test1_x_component = test1_data[:,0] + np.random.normal(0, _std_dev_noise, test1_sensors_total_num)\n",
    "        test1_y_component = test1_data[:,1] + np.random.normal(0, _std_dev_noise, test1_sensors_total_num)\n",
    "        #test1_x_component = add_noise_to_data(test1_data[:,0], test1_sensors_total_num)\n",
    "        #test1_y_component = add_noise_to_data(test1_data[:,1], test1_sensors_total_num)\n",
    "        # Data stored in the form of XYXY components.\n",
    "        displacement_model[:,counter] = np.vstack((test1_x_component, test1_y_component)).T.flatten()\n",
    "\n",
    "    displacement_model = displacement_model.flatten('F')\n",
    "\n",
    "    return -0.5 * np.sum((displacement_data - displacement_model) ** 2 / _std_dev_noise**2 + np.log(_std_dev_noise**2))\n",
    "\n",
    "def log_prior(theta):\n",
    "    _lp = 0\n",
    "    # This loop reads the parameters from the json file and calulates the log prior.\n",
    "    for index, parameter in enumerate(json_object.get('parameters')):\n",
    "        if parameter['prior'][0] == 'Normal': \n",
    "            _lp += norm.logpdf(theta[index], loc = parameter['prior'][1][\"mean\"], scale = parameter['prior'][1][\"std_dev\"]) \n",
    "\n",
    "        elif parameter['prior'][0] == 'Uniform':   \n",
    "            _lp += uniform.logpdf(theta[index], loc = parameter['prior'][1][\"lower_bound\"], scale = parameter['prior'][1][\"lower_bound\"] + parameter['prior'][1][\"upper_bound\"])    \n",
    "        \n",
    "        #if parameter['prior'][0] == 'Bernoulli':\n",
    "        #    ## Uncomment the following lines to see the change in trace of the Bernoulli parameters.\n",
    "        #    #if theta[index] >= 0.3:    # Trial 1\n",
    "        #    #    theta[index] = 1\n",
    "        #    #else:\n",
    "        #    #    theta[index] = 0\n",
    "        #    if theta[index] < 0.0 or theta[index] > 1.0:\n",
    "        #        return -np.inf\n",
    "        #    _lp += bernoulli.logpmf(0 if theta[index] < 0.5 else 1, p = parameter['prior'][1][\"p\"])            \n",
    "        #elif parameter['prior'][0] == 'Spike-Slab':\n",
    "        #    for hyperparameter in parameter['hyperparameters']:\n",
    "        #        for ind, param in enumerate(json_object.get('parameters')):\n",
    "        #            if hyperparameter == param['name']:\n",
    "        #                lmbda = theta[ind]\n",
    "        #                break\n",
    "        #    if lmbda >= 0.5:\n",
    "        #        _lp += norm.logpdf(theta[index], loc = parameter['prior'][1][\"mean\"], scale = parameter['prior'][1][\"std_dev\"])\n",
    "        #    else: # lmbda < 0.5: #lmbda won't be less than 0 or greater than 1. That scneario is already taken care of in the Bernoulli if statement.\n",
    "        #        theta[index] = 0\n",
    "    return _lp\n",
    "\n",
    "def log_probability(theta, displacement_data, _std_noise, temperature_functions):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, displacement_data, _std_noise, temperature_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev_noise = 1e-5\n",
    "import emcee\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, log_probability, args=(displacement_data, std_dev_noise, temperature_functions))\n",
    "\n",
    "#filename = \"posterior_snapshots.h5\"\n",
    "#backend = emcee.backends.HDFBackend(filename)\n",
    "#backend.reset(nwalkers, ndim)\n",
    "\n",
    "sampler.run_mcmc(start_parameters, json_object.get('MCMC').get('nsteps'), progress=True)\n",
    "samples = sampler.get_chain()\n",
    "np.savetxt(json_object.get('MCMC').get('posterior'), samples.reshape(samples.shape[0], -1), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_data = np.loadtxt(json_object.get('MCMC').get('posterior'), delimiter=',')\n",
    "posterior = chain_data.reshape(chain_data.shape[0], chain_data.shape[1]// ndim, ndim)\n",
    "posterior =posterior[:,:,:]\n",
    "# chain_state/step number, chain_index, parameter_index\n",
    "labels = json_object.get('MCMC').get('reformulated_param_list') # Change the labels over here if changes in parameters are made in json file.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig1, axes = plt.subplots(ndim, figsize=(10, 7), sharex=True)\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(posterior[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(posterior))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "flat_samples = posterior.reshape(-1, ndim)\n",
    "fig2 = corner.corner(\n",
    "    flat_samples, labels=labels, truths=json_object.get('MCMC').get('true_values')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting with rounded off parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for arr_ind_i, matrix in enumerate(posterior):\n",
    "    for arr_ind_j, row in enumerate(matrix):\n",
    "        #print(row.shape)\n",
    "        for arr_ind_k, element in enumerate(row[0:-1:2]):\n",
    "                #print(arr_ind_k)\n",
    "                #if arr_ind_k%2 == 0 and element < 0.5:\n",
    "                if element < 0.5:\n",
    "                    posterior[arr_ind_i,arr_ind_j,2*arr_ind_k] = 0\n",
    "                    posterior[arr_ind_i,arr_ind_j,2*arr_ind_k+1] = 0\n",
    "                else:\n",
    "                    posterior[arr_ind_i,arr_ind_j,2*arr_ind_k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3, axes = plt.subplots(ndim, figsize=(10, 7), sharex=True)\n",
    "labels = json_object.get('MCMC').get('param_list')\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(posterior[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(posterior))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = posterior.reshape(-1, ndim)\n",
    "fig4 = corner.corner(\n",
    "    flat_samples, labels=labels, truths=json_object.get('MCMC').get('true_values')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is for comparison with the other stored chains and doing analysis using plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.linspace(15,35,5)\n",
    "s = 235-0.04*T**2\n",
    "chain_idx = -1\n",
    "walker_idx = 2\n",
    "Youngs_Mod = 235 + scaler*posterior[chain_idx, walker_idx, 1]*T**2 + posterior[chain_idx, walker_idx, 3]*T + posterior[chain_idx, walker_idx, 5]*np.log(T) + posterior[chain_idx, walker_idx, 7]*np.exp(-T)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(T, s, marker='o')\n",
    "ax.plot(T, Youngs_Mod)\n",
    "\n",
    "ax.set(xlabel='Temp (s)', ylabel='E ',\n",
    "       title='Youngs Modulus vs Temperature')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = sampler.get_chain(discard=1000, thin=15, flat=True)\n",
    "sum(posterior[:,:,0] > 0.5)/500"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenicsxclone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
